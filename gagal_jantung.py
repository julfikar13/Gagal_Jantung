# -*- coding: utf-8 -*-
"""Gagal_Jantung.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g8aZ-xPSbkAKJbfUt0HTzQKoyC8eWuzs
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

from sklearn.metrics import confusion_matrix , classification_report

from google.colab import drive
df=pd.read_csv('/content/drive/MyDrive/Dataset gagal jantung/heart_failure_clinical_records_dataset.csv')

df.head()

df.tail()

df['serum_creatinine']=df['serum_creatinine'].astype(int)

df.info()

df.dtypes

print("jumlah baris dalam dataset adalah:", df.shape[0], "baris")
print("jumlah kolom dalam dataset adalah:",df.shape[1], "kolom")

"""data prepocessing"""

#mengganti nama kolom menjadi bahasa indonesia
df.rename(columns={'age': 'Usia',
                   'anaemia': 'Anemia',
                   'creatinine_phosphokinase': 'Enzim',
                   'diabetes': 'Diabetes',
                   'ejection_fraction': 'Ef(%)',
                   'high_blood_pressure': 'Tekanan_Darah_Tinggi',
                   'platelets': 'Trombosit',
                   'sex':'Jenis_Kelamin',
                   'smoking':'Perokok',
                   'time':'Waktu',
                   'DEATH_EVENT':'Kematian',}, inplace=True)

#Mengecek nama kolom dari bahasa inggris menjadi bahasa indonesia
df.head()

df['Usia']=df['Usia'].astype(int)

df['Trombosit']=df['Trombosit'].astype(int)

#Menghapus data yang di duplikasi
df=df.drop_duplicates()

df.info()

#Melihat jumlah kelas 0 dan 1
print(df['Kematian'].value_counts())
cls_0=df[df['Kematian']==0]
cls_1=df[df['Kematian']==1]

#Melihat jumlah kelas 0 dan 1(sampling data)

cls_0=cls_0.sample(500,replace=True)
cls_1=cls_1.sample(500,replace=True)
df=pd.concat([cls_0,cls_1],axis=0)
df.info()

df.isnull().sum()

df.describe()

#PENGECEKAN PERSEBARAN DATA 
df.hist(figsize=(15,15))
plt.show()

#MEMBUAT TABEL KORELASI
sns.heatmap(df.corr(),annot=True, cmap='terrain', linewidths=0.1)
fig=plt.gcf()
fig.set_size_inches(10,6)
plt.show()

df.boxplot(figsize=(18S,10))

#df = sns.load_dataset("titanic")
sns.boxplot(x=df["Usia"])

sns.boxplot(x=df["Enzim"])

sns.boxplot(x=df["Ef(%)"])

sns.boxplot(x=df["Trombosit"])

sns.boxplot(x=df["serum_creatinine"])

sns.boxplot(x=df["serum_sodium"])

from abc import ABC
from collections.abc import Mapping
from dataclasses import dataclass, field
import plotly.graph_objects as go
from plotly.subplots import make_subplots
d1 = df[(df["Perokok"]==0) & (df["Jenis_Kelamin"]==1)]
d2 = df[(df["Perokok"]==1) & (df["Jenis_Kelamin"]==1)]
d3 = df[(df["Perokok"]==0) & (df["Jenis_Kelamin"]==0)]
d4 = df[(df["Perokok"]==1) & (df["Jenis_Kelamin"]==0)]
label1 = ["Pria","Wanita"]
label2 = ['Pria - Hidup ','Pria - Mati', "Wanita -  Hidup", "Wanita - Mati"]
values1 = [(len(d1)+len(d2)), (len(d3)+len(d4))]
values2 = [len(d1),len(d2),len(d3),len(d4)]
# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=label1, values=values1, name="GENDER"),
              1, 1)
fig.add_trace(go.Pie(labels=label2, values=values2, name="GENDER VS DEATH_EVENT"),
              1, 2)

# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent")

fig.update_layout(
    title_text="GENDER DISTRIBUTION IN THE DATASET  \
                   Jenis_Kelamin VS Perokok",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Jenis_Kelamin', x=0.19, y=0.5, font_size=10, showarrow=False),
                 dict(text='Jenis_Kelamin VS Perokok', x=0.84, y=0.5, font_size=9, showarrow=False)],
    autosize=False,width=1200, height=500, paper_bgcolor="white")

fig.show()

cols_to_scale = df.columns

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

df.head()

"""splitting test

"""

X=df.drop(['Waktu','Kematian'],axis=1)
Y=df['Kematian']

X.head()

#import library data partitioning/splitting data dan konfigurasi
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.2, random_state=101,shuffle=True)

#import library algoritma
from sklearn import svm
from sklearn.metrics import accuracy_score

svmc =svm.SVC()
svmc.fit(X_train,Y_train)

Y_pred_svmc = svmc.predict(X_test)


accuracy_svmc=accuracy_score(Y_test,Y_pred_svmc)*100
accuracy_svmc

print("accuracy on training set: {:.3f}". format(svmc.score(X_train,Y_train)))
print("accuracy on test set: {:.3f}".format(svmc.score(X_test,Y_test)))

#import library random foresd
from sklearn.ensemble import RandomForestClassifier

rf= RandomForestClassifier()
rf.fit(X_train,Y_train)

Y_pred_rf = rf.predict(X_test)

accuracy_rf=accuracy_score(Y_test,Y_pred_rf)*100
accuracy_rf

print("accuracy on training set: {:.3f}". format(rf.score(X_train,Y_train)))
print("accuracy on test set: {:.3f}".format(rf.score(X_test,Y_test)))

#import library GradientBoostingClassifier
from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()
gbc.fit(X_train,Y_train)

Y_pred_gbc = gbc.predict(X_test)

accuracy_gbc=accuracy_score(Y_test,Y_pred_rf)*100
accuracy_gbc

print("accuracy on training set: {:.3f}". format(gbc.score(X_train,Y_train)))
print("accuracy on test set: {:.3f}".format(gbc.score(X_test,Y_test)))

#import library KNN
from sklearn.neighbors import KNeighborsClassifier

knn= KNeighborsClassifier()
knn.fit(X_train,Y_train)

Y_pred_knn = knn.predict(X_test)

accuracy_knn=accuracy_score(Y_test,Y_pred_rf)*100
accuracy_knn

print("accuracy on training set: {:.3f}". format(knn.score(X_train,Y_train)))
print("accuracy on test set: {:.3f}".format(knn.score(X_test,Y_test)))

"""MODEL OPTIMIZATION

OPTIMALISASI SVM
"""

# IMPORT LIBRARY GRIDSEARCHCV UNTUK HYPER-PARAMETER TUNING
from sklearn.model_selection import GridSearchCV
  
# DEKLARASI PARAMETER YANG AKAN DIPAKAI BESERTA VALUENYA
param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 

# INISIALISASI HYPER-TUNING
grid_svm = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)
  
# FITTING MODEL RF DENGAN HYPER TUNING
grid_svm.fit(X_train, Y_train)

# PARAMETER TERBAIK RF DALAM STUDI KASUS INI
print("Parameter Terbaik Setelah Tuning :\n", grid_svm.best_params_)
print("Model Setelah Hyper-Parameter Tuning :\n", grid_svm.best_estimator_)

# MELAKUKAN PREDIKSI DENGAN MODEL HYPER-PARAMETER
y_pred_grid_svm = grid_svm.predict(X_test)

# MELIHAT AKURASI MODEL
accuracy_grid_svmc = accuracy_score(Y_test,y_pred_grid_svm)*100
accuracy_grid_svmc

print("Accuracy on training set: {:.3f}".format(grid_svm.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(grid_svm.score(X_test, Y_test)))

"""OPTIMALISASI Random Forest"""

# MENCARI LIST PARAMETER PADA RANDOM FOREST
rf.get_params().keys()

# IMPORT LIBRARY GRIDSEARCHCV UNTUK HYPER-PARAMETER TUNING
from sklearn.model_selection import GridSearchCV

# DEKLARASI PARAMETER YANG AKAN DIPAKAI BESERTA VALUENYA
param_grid = { 
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
}

# INISIALISASI HYPER-TUNING
grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)

# FITTING MODEL RF DENGAN HYPER TUNING
grid_rf.fit(X_train, Y_train)

# PARAMETER TERBAIK RF DALAM STUDI KASUS INI
print("Parameter Terbaik Setelah Tuning :\n", grid_rf.best_params_)
print("Model Setelah Hyper-Parameter Tuning :\n", grid_rf.best_estimator_)

# MELAKUKAN PREDIKSI DENGAN MODEL HYPER-PARAMETER
y_pred_grid_rf = grid_rf.predict(X_test)

# MELIHAT AKURASI MODEL
accuracy_grid_rf = accuracy_score(Y_test,y_pred_grid_rf)*100
accuracy_grid_rf

print("Accuracy on training set: {:.3f}".format(grid_rf.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(grid_rf.score(X_test, Y_test)))

"""OPTIMALISASI Gradient Boosting"""

gbc.get_params().keys()

from sklearn.model_selection import GridSearchCV

param_grid = { 
    "loss":["deviance"],
    "learning_rate": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
    "min_samples_split": np.linspace(0.1, 0.5, 12),
    "min_samples_leaf": np.linspace(0.1, 0.5, 12),
    "max_depth":[3,5,8],
    "max_features":["log2","sqrt"],
    "criterion": ["friedman_mse",  "mae"],
    "subsample":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],
    "n_estimators":[10]
}

grid_gbc = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=10, n_jobs=-1)

# fitting the model for grid search
grid_gbc.fit(X_train, Y_train)

"""KESIMPULAN"""

print("Akurasi SVM Pada Dataset Training: {:.3f}".format(grid_svm.score(X_train, Y_train)))
print("Akurasi SVM Pada Dataset Testing: {:.3f}".format(grid_svm.score(X_test, Y_test)))

print("Akurasi RF Pada Dataset Training: {:.3f}".format(grid_rf.score(X_train, Y_train)))
print("Akurasi RF Pada Dataset Testing: {:.3f}".format(grid_rf.score(X_test, Y_test)))

print("Akurasi KNN Pada Dataset Training: {:.3f}".format(knn.score(X_train, Y_train)))
print("Akurasi KNN Pada Dataset Testing: {:.3f}".format(knn.score(X_test, Y_test)))

#print("Akurasi GBC Pada Dataset Training: {:.3f}".format(grid_gbc.score(X_train, Y_train)))
#print("Akurasi GBC Pada Dataset Testing: {:.3f}".format(grid_gbc.score(X_test, Y_test)))

"""RANDOM FORES"""

# MELAKUKAN PREDIKSI DENGAN MODEL HYPER-PARAMETER
y_pred_grid_rf = grid_rf.predict(X_test)

# MELIHAT AKURASI MODEL
accuracy_grid_rf = accuracy_score(Y_test,y_pred_grid_rf)*100
accuracy_grid_rf

print("Accuracy on training set: {:.3f}".format(grid_rf.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(grid_rf.score(X_test, Y_test)))

"""KNN"""

# MELAKUKAN PREDIKSI DENGAN MODEL HYPER-PARAMETER
y_pred_knn = knn.predict(X_test)

# MELIHAT AKURASI MODEL
accuracy_knn = accuracy_score(Y_test,y_pred_knn)*100
accuracy_knn

print("Accuracy on training set: {:.3f}".format(knn.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(knn.score(X_test, Y_test)))

#MODEL EVALUATION
# IMPORT LIBRARY EVALUATION
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import classification_report

"""svm

"""

# MEMBUAT CONFUSION MATRIX
grid_svm_matrix = confusion_matrix(Y_test, y_pred_grid_svm)

# VISUALISASI CONFUSION MATRIX
sns.heatmap(grid_svm_matrix, annot=True)

"""random fores"""

# MEMBUAT CONFUSION MATRIX RANDOM FORES
grid_rf_matrix = confusion_matrix(Y_test, y_pred_grid_rf)

# VISUALISASI CONFUSION MATRIX
sns.heatmap(grid_rf_matrix, annot=True)

"""knn"""

# MEMBUAT CONFUSION MATRIX
knn_matrix = confusion_matrix(Y_test, y_pred_knn)

# VISUALISASI CONFUSION MATRIX
sns.heatmap(knn_matrix, annot=True)

"""svm"""

# MEMBUAT KURVA ROC-AUC

svm_auc = roc_auc_score(Y_test, y_pred_grid_svm)
print('SVM: ROC AUC=%.3f' % (svm_auc))

# KALKULASI KURVA ROC-AUC

svm_fpr, svm_tpr, _ = roc_curve(Y_test, y_pred_grid_svm)

# VISUALISASI KURVA ROC-AUC

plt.plot(svm_fpr, svm_tpr, marker='.', label='SVM')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

"""random fores"""

# MEMBUAT KURVA ROC-AUC
rf_auc = roc_auc_score(Y_test, y_pred_grid_rf)
print('RF: ROC AUC=%.3f' % (rf_auc))

# KALKULASI KURVA ROC-AUC

rf_fpr, rf_tpr, _ = roc_curve(Y_test, y_pred_grid_rf)

# VISUALISASI KURVA ROC-AUC

plt.plot(rf_fpr, rf_tpr, marker='.', label='RF')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

"""knn"""

# MEMBUAT KURVA ROC-AUC
knn_auc = roc_auc_score(Y_test, y_pred_knn)
print('RF: ROC AUC=%.3f' % (rf_auc))

# KALKULASI KURVA ROC-AUC

knn_fpr, knn_tpr, _ = roc_curve(Y_test, y_pred_knn)

# VISUALISASI KURVA ROC-AUC

plt.plot(knn_fpr, knn_tpr, marker='.', label='RF')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

"""SVM"""

# MEMBUAT CLASSIFICATION REPORT MODEL SVM
print(classification_report(y_pred_grid_svm, Y_test))

"""RANDOM FORES"""

# MEMBUAT CLASSIFICATION REPORT MODEL rf
print(classification_report(y_pred_grid_rf, Y_test))

"""KNN"""

# MEMBUAT CLASSIFICATION REPORT MODEL SVM
print(classification_report(y_pred_knn, Y_test))